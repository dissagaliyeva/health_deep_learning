{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f624fc56-2b40-4e45-a9fe-04ac480db4dd",
   "metadata": {},
   "source": [
    "# Lab 1: Getting started with PyTorch\n",
    "\n",
    "The first lab to introduce PyTorch basics including `Tensor`, `Loss`, and `Adagrad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cebf341-41b8-4cd6-a83c-54c50785ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a58228f5-0563-4dc6-9bbe-066a25222565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502d7f5-20da-4cde-8c39-cdc63079b0fa",
   "metadata": {},
   "source": [
    "## 1. Tensors\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators. Tensors are also optimized for automatic differentiation (we’ll see more about that later in the Autograd section). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a5f5a2-d686-43a9-afa9-a3bbbe90f8d3",
   "metadata": {},
   "source": [
    "### 1.1 Initializing a Tensor\n",
    "\n",
    "Tensors can be initialized in various ways. Take a look at the following examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0717e8a-3368-4bd7-9f2b-948e645a9411",
   "metadata": {},
   "source": [
    "**Directly from data**\n",
    "\n",
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21af4757-4d06-4f92-a14a-c7fdf8c4f2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f4726-7982-42ac-a52e-68d367c116e8",
   "metadata": {},
   "source": [
    "**From a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfea29f6-45f7-483d-af9b-965ea87a101d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419cdb76-27e1-43b5-88f4-a3078465a5c6",
   "metadata": {},
   "source": [
    "**From another tensor:**\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6437c5d4-f690-4a9a-8e77-86375e44898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.8823, 0.9150],\n",
      "        [0.3829, 0.9593]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97ff26-4a41-4545-ba7f-5b11529eff16",
   "metadata": {},
   "source": [
    "**With random or constant values:**\n",
    "\n",
    "``shape`` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d8574c-35dc-407d-a173-d14f8735568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor: tensor([[0.3904, 0.6009, 0.2566],\n",
      "        [0.7936, 0.9408, 0.1332]])\n",
      "Ones tensor: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Zeros tensor: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zero_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f'Random tensor: {rand_tensor}')\n",
    "print(f'Ones tensor: {ones_tensor}')\n",
    "print(f'Zeros tensor: {zero_tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae3e3e3-1533-448f-aafe-b3aa42ddac98",
   "metadata": {},
   "source": [
    "### 1.2 Attributes of a Tensor\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3863d0c2-cdbf-46d4-bd18-fa8edd012b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([3, 4])\n",
      "Datatype: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print('Tensor shape:', tensor.shape)\n",
    "print('Datatype:', tensor.dtype)\n",
    "print('Device tensor is stored on:', tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03532a82-893e-419c-9c60-315913c240ec",
   "metadata": {},
   "source": [
    "### 1.3 Operations on Tensors\n",
    "\n",
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, \n",
    "indexing, slicing), sampling and more are\n",
    "comprehensively described [here](https://pytorch.org/docs/stable/torch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9783c2fa-917d-4169-801d-295b076b4ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "First row: tensor([0., 1., 2., 3.])\n",
      "First column: tensor([0., 4., 8.])\n",
      "Last column: tensor([ 3.,  7., 11.])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(12).reshape(3, 4).float()\n",
    "\n",
    "print(tensor)\n",
    "print('First row:', tensor[0])\n",
    "print('First column:', tensor[:, 0])\n",
    "print('Last column:', tensor[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f51309-34ac-4d50-9b48-5e432a1b9b42",
   "metadata": {},
   "source": [
    "**Joining tensors** \n",
    "\n",
    "You can use ``torch.cat`` to concatenate a sequence of tensors along a given dimension.\n",
    "See also [`torch.stack`](https://pytorch.org/docs/stable/generated/torch.stack.html),\n",
    "another tensor joining op that is subtly different from ``torch.cat``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "480d6a83-6e62-4c8b-9610-7417a66ae7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  0.,  1.,  2.,  3.,  0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  4.,  5.,  6.,  7.,  4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.,  8.,  9., 10., 11.,  8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f16483ff-e3e3-4124-9536-2d4780cb4936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077bac26-7665-4f57-a0d8-8b2334c6cbb6",
   "metadata": {},
   "source": [
    "#### Arithmetic operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b53c831-a3dd-42f3-8883-042fcf54c3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 14.,  38.,  62.],\n",
      "        [ 38., 126., 214.],\n",
      "        [ 62., 214., 366.]])\n",
      "tensor([[ 14.,  38.,  62.],\n",
      "        [ 38., 126., 214.],\n",
      "        [ 62., 214., 366.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8098/159678247.py:9: UserWarning: An output with one or more elements was resized since it had shape [3, 4], which does not match the required output shape [3, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  torch.matmul(tensor, tensor.T, out=y3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 14.,  38.,  62.],\n",
       "        [ 38., 126., 214.],\n",
       "        [ 62., 214., 366.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "y1 = tensor @ tensor.T\n",
    "print(y1)\n",
    "\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "print(y2)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "184cb052-1f51-4daf-94c0-6fd9ea642aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.]])\n",
      "tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.]])\n",
      "tensor([[  0.,   1.,   4.,   9.],\n",
      "        [ 16.,  25.,  36.,  49.],\n",
      "        [ 64.,  81., 100., 121.]])\n"
     ]
    }
   ],
   "source": [
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "print(z1)\n",
    "z2 = tensor.mul(tensor)\n",
    "print(z2)\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee09e8-162f-4b1c-9077-20aeb06a4f2e",
   "metadata": {},
   "source": [
    "**Single-element tensors** \n",
    "\n",
    "If you have a one-element tensor, for example by aggregating all\n",
    "values of a tensor into one value, you can convert it to a Python\n",
    "numerical value using ``item()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ed74786-05e8-410d-8ccc-b1b21a7cd1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()  \n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac8dce-47d5-4378-9aad-f78b4ea306b7",
   "metadata": {},
   "source": [
    "### 1.4 GPU Acceleration\n",
    "\n",
    "If we have NVIDIA GPU(s), we can accelerate computation once we move Tensors onto GPU.\n",
    "Let's compare how much GPU can accelerate especially matrix operations.\n",
    "We will do a matrix-matrix multiplication between two 5k-by-5k matrices on both CPU and GPU.\n",
    "\n",
    "Unfortunately, Coursera does not have a GPU environment. But feel free to try the following snippets on a GPU machine. Ideally, with GPU acceleration, matrix multiplication will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "335f4bd6-5b5b-4f30-9d2b-a3c69e18d3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7886, 0.5895, 0.7539,  ..., 0.9313, 0.6453, 0.9844],\n",
       "        [0.8312, 0.8682, 0.9359,  ..., 0.4046, 0.9987, 0.8608],\n",
       "        [0.1268, 0.2253, 0.1223,  ..., 0.3939, 0.4493, 0.5327],\n",
       "        ...,\n",
       "        [0.5851, 0.5824, 0.8857,  ..., 0.3165, 0.4845, 0.3896],\n",
       "        [0.3348, 0.1535, 0.5840,  ..., 0.0285, 0.7444, 0.5193],\n",
       "        [0.4027, 0.7190, 0.3847,  ..., 0.9606, 0.6629, 0.0359]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.rand(5000, 5000)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47e6ddd2-6b22-4f56-b9cb-923a3bdb52ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.86 s, sys: 46.1 ms, total: 4.9 s\n",
      "Wall time: 821 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1697.6150, 1260.7886, 1260.5798,  ..., 1276.7966, 1265.4301,\n",
       "         1273.4854],\n",
       "        [1260.7886, 1655.7583, 1247.6844,  ..., 1256.2369, 1263.6820,\n",
       "         1263.9419],\n",
       "        [1260.5798, 1247.6844, 1658.2700,  ..., 1258.1453, 1267.2045,\n",
       "         1274.8801],\n",
       "        ...,\n",
       "        [1276.7966, 1256.2367, 1258.1453,  ..., 1704.4204, 1270.7224,\n",
       "         1275.9523],\n",
       "        [1265.4302, 1263.6820, 1267.2046,  ..., 1270.7224, 1686.0026,\n",
       "         1272.0712],\n",
       "        [1273.4852, 1263.9419, 1274.8801,  ..., 1275.9523, 1272.0712,\n",
       "         1699.9994]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "torch.mm(mat.t(), mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2713ee0d-de15-4d78-96ef-b813423cc14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available!\n",
      "CPU times: user 1.75 ms, sys: 189 µs, total: 1.94 ms\n",
      "Wall time: 2.95 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if torch.cuda.is_available():\n",
    "    mat = mat.cuda()\n",
    "    torch.mm(mat.t(), mat)\n",
    "else:\n",
    "    print('GPU is not available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9a738d-1a1f-4c17-9372-30c5cc6d6748",
   "metadata": {},
   "source": [
    "### Exercise 1 [10 points]\n",
    "\n",
    "Implement the Sigmoid function on your own.\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "Note that you should not use existing PyTorch implementation.\n",
    "\n",
    "Hint: try `torch.exp()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "848d536b-e36b-42f4-b2a1-9f02186a0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return torch.div(1, 1 + torch.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cc1e023-29a0-45f5-98f3-b7f5a67b30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "assert torch.allclose(sigmoid(torch.tensor([1.2])), torch.tensor([0.7685]), rtol=1e-2)\n",
    "assert torch.allclose(sigmoid(torch.tensor([0, 1.5])), torch.tensor([0.5000, 0.8176]), rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6bfa5e-8256-40eb-bed5-4dd4c296356b",
   "metadata": {},
   "source": [
    "### Exercise 2 [10 points]\n",
    "\n",
    "Implement a Softmax function on your own.\n",
    "\n",
    "$$\\mathrm{softmax}(\\mathbf{X})_{ij} = \\frac{\\exp(\\mathbf{X}_{ij})}{\\sum_k \\exp(\\mathbf{X}_{ik})}$$\n",
    "\n",
    "Note that you should not use existing PyTorch implementation.\n",
    "\n",
    "Hint: try `torch.exp()` and `torch.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5526a4e1-22cb-43ca-8051-7e84b46d72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7723ba1a-527d-44c3-a911-37b57b8ac0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3304, 0.3965, 0.2731],\n",
       "        [0.4523, 0.2515, 0.2962]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.2288, 0.4111, 0.0385], [0.6233, 0.0364, 0.1999]])\n",
    "\n",
    "softmax(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1a533a4-b611-4380-a284-1e1042e88b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "X = torch.tensor([[0.2288, 0.4111, 0.0385], [0.6233, 0.0364, 0.1999]])\n",
    "assert torch.allclose(softmax(X), torch.tensor([[0.3304, 0.3965, 0.2731], [0.4523, 0.2515, 0.2962]]), rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1491df7-f234-489e-a30a-e2730cd982da",
   "metadata": {},
   "source": [
    "### Exercise 3 [10 points]\n",
    "\n",
    "Implement a linear layer.\n",
    "\n",
    "$$\\mathbf{O} = \\mathbf{X}\\mathbf{W} + \\mathbf{b},$$\n",
    "\n",
    "where $\\mathbf{X}$ is the input feature, $\\mathbf{O}$ is the output feature, $\\mathbf{W}$ and $\\mathbf{b}$ are the weight parameters.\n",
    "\n",
    "Hint: try `torch.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c12208d6-193c-431b-b6e0-abf55605b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(X, W, b):\n",
    "    return torch.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35d4ae59-4d5a-43ed-9aa5-6272e55c4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "AUTOGRADER CELL. DO NOT MODIFY THIS.\n",
    "'''\n",
    "\n",
    "X = torch.Tensor([[0.1, 0.2, 0.3]])\n",
    "W = torch.Tensor([[0.1, 0.2, 0.3]]).T\n",
    "b = torch.Tensor([-0.5])\n",
    "assert torch.allclose(linear(X, W, b), torch.Tensor([[-0.3600]]), rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32214376-e086-4243-94a1-cb69238b513d",
   "metadata": {},
   "source": [
    "## 2. Loss\n",
    "\n",
    "When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67027d97-51f2-402a-ba48-2502844731e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
